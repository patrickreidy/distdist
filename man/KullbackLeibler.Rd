% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/distances.R
\name{KullbackLeibler}
\alias{KullbackLeibler}
\alias{KullbackLeibler,Distribution,Distribution-method}
\title{Kullback-Leibler Divergence}
\usage{
\S4method{KullbackLeibler}{Distribution,Distribution}(p1, p2)
}
\arguments{
\item{p1, p2}{\code{\link[=Distribution-class]{Distributions}}.}
}
\value{
The Kullback-Leibler divergence of \code{p2} from \code{p1}.
}
\description{
Compute the Kullback-Leibler divergence of one probability distribution
from another. This divergence is also known as the relative entropy, the
information deviation, and the information gain.
}
\details{
Let \code{p1} and \code{p2} denote the vectors of probability mass assigned
by two distributions defined on the same state space; furthermore, let
these distributions be strictly positively-valued. Then, the Kullback-Leibler
divergence of \code{p2} from \code{p1} is given by \code{sum(p1 * log(p1 / p2))}.

Note that the terminology "divergence of \code{p2} from \code{p1}" indicates
that \code{p1} is the reference distribution against which the distribution
\code{p2} is evaluated.

Kullback-Leibler divergence is not a symmetric function. That is, it is not
generally true that \code{KullbackLeibler(p1, p2) = KullbackLeibler(p2, p1)}.
Symmetric functions based on the Kullback-Leibler divergence are available
through the Jeffrey and Topsoe distance.
}
\seealso{
\code{\link[=JensenShannon]{Jensen-Shannon}}, \code{\link{Jeffrey}}, \code{\link{Topsoe}}
}
